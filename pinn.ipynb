{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ac907-942f-4eb3-a3c0-1145bdaf64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import torch\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb778c36-a09d-4b0a-b660-73ddc5c3b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define our data points.\n",
    "Nbd = 100\n",
    "N = 5000\n",
    "\n",
    "# Our boundary conditions are u(x,0) = -sin(pi x); u(-1,t) = u(1,t) = 0\n",
    "xt_bd = np.vstack((\n",
    "    np.vstack((np.linspace(-1,1,Nbd),np.zeros(Nbd))).transpose(),\n",
    "    np.vstack((-np.ones(Nbd),np.linspace(1/Nbd,1,Nbd))).transpose(),\n",
    "    np.vstack((np.ones(Nbd),np.linspace(1/Nbd,1,Nbd))).transpose()\n",
    "),dtype=np.float32)\n",
    "u_bd = np.hstack((\n",
    "    -np.sin(np.pi*np.linspace(-1,1,Nbd)),\n",
    "    np.zeros(2*Nbd)\n",
    "),dtype=np.float32)\n",
    "ν = 0.01\n",
    "\n",
    "# We sample interior points with a latin hypercube.\n",
    "sampler = LatinHypercube(2)\n",
    "xt = sampler.random(n=N)\n",
    "xt[:,0] = 2*xt[:,0]-1\n",
    "\n",
    "xt = np.vstack((xt_bd,xt),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5738f31-ecd4-40aa-8103-a2097c6c5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom loss function is the core of the PINN.\n",
    "def lossfn(y_true,y_pred):\n",
    "    # Where we have Dirischlet boundary conditions, we can just use that error\n",
    "    bd_loss = torch.sum(keras.losses.mean_squared_error(y_true,y_pred))/(3*Nbd)\n",
    "\n",
    "    # Run the model forward and watch the derivatives with respect to (x,t).\n",
    "    # We make sure to maintain the graph because we'll need it again for the\n",
    "    # second derivatives.\n",
    "    # By calling autograd.grad() instead of just u.backward(), we can avoid\n",
    "    # taking the derivatives of weights on this pass since we don't need them\n",
    "    xt_tensor = torch.tensor(xt,requires_grad=True, device=y_pred.device)\n",
    "    xt_tensor.grad = None\n",
    "    u = model(xt_tensor).squeeze()\n",
    "    xt_grad = torch.autograd.grad(\n",
    "        u,xt_tensor,grad_outputs=torch.ones(u.shape,device=u.device),\n",
    "        retain_graph=True,create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    du_dx = xt_grad[:,0]\n",
    "    du_dt = xt_grad[:,1]\n",
    "    xt_grad2 = torch.autograd.grad(\n",
    "        du_dx,xt_tensor,grad_outputs=torch.ones(u.shape,device=u.device),\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    d2u_dx2 = xt_grad2[:,0]\n",
    "\n",
    "    # compute the physical loss\n",
    "    residual = du_dt + u * du_dx - ( ν / np.pi) * d2u_dx2\n",
    "    phys_loss = torch.sum(torch.pow(residual,2))/N\n",
    "\n",
    "    # Weight the physical and boundary loss with a hyperparameter weighting\n",
    "    return 5*bd_loss + phys_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068095a9-6ba1-4a2b-974f-fe30d08757ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deep neural network with 8 size-20 layers\n",
    "nnlayers = [20,20,20,20,20,20,20,20]\n",
    "model = keras.Sequential([])\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "for L in nnlayers:\n",
    "    model.add(layers.Dense(L, activation='tanh'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss=lossfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a2878-b547-4cbc-a904-d318f5e15170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, this is where we must leave keras behind\n",
    "# and write a torch-style training loop\n",
    "\n",
    "def run_epoch(model, input, target):\n",
    "    # Doing this with a closure() function isn't necessary for\n",
    "    # most optimizers, but LBFGS needs it and that's the best one\n",
    "    # for PINNs usually, so it's good practice.\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = lossfn(target,output)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    loss = optimizer.step(closure)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f88de8-4873-4463-824f-26d870c5394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "patience = 10\n",
    "threshold = 1e-4\n",
    "\n",
    "losses = np.array([0.]*epochs)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "bar = tqdm(range(epochs))\n",
    "for e in bar:\n",
    "    model.train(True)\n",
    "    loss = run_epoch(model,xt_bd,u_bd)\n",
    "    losses[e] = loss\n",
    "    bar.set_description(f'epoch {e+1}, loss: {loss:.3e}')\n",
    "    \n",
    "    if e > patience and np.max(\n",
    "        np.abs(losses[e-patience:e]-loss)\n",
    "    )<threshold*loss:\n",
    "        print('Model converged.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b6743-dfd7-4ed6-93be-6066b2069df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses[0:e])\n",
    "plt.title('Training convergence');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12edfb-1bbd-4ed4-a283-9b209703a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the model for the whole domain and see what it looks like\n",
    "x_full, t_full = np.meshgrid(np.linspace(-1,1,512),np.linspace(0,1,1024))\n",
    "xt_full = np.vstack((x_full.ravel(),t_full.ravel())).transpose()\n",
    "u_full = model.predict(xt_full,batch_size=10000)\n",
    "\n",
    "u_full = np.reshape(u_full,(1024,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13edc3-9420-4113-9ff0-ccc2da611937",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(3)\n",
    "axs[0].plot(x_full[-1,:],u_full[-1,:])\n",
    "axs[1].imshow(u_full,origin='lower',extent=[-1, 1, 0, 1])\n",
    "axs[2].plot(x_full[0,:],u_full[0,:])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93559586-b7c2-4348-a4a9-79a35c947b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
