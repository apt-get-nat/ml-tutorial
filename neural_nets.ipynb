{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328723b-0ff5-46cf-8cae-3eafc2829b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import torch\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8557b82-235d-4e42-875d-affe935be89c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data and split off some for testing\n",
    "data = load_breast_cancer()\n",
    "x = data['data'][:-10]\n",
    "y = data['target'][:-10]\n",
    "x_test = data['data'][-10:]\n",
    "# There are only two categories here. If there are more, you often\n",
    "# need to convert the target from the format it's served in where categories\n",
    "# are [0,1,2,...] to a 1-hot format, ie [1,0,0,...], [0,1,0,...], [0,0,1,...], ...\n",
    "y_test = data['target'][-10:]\n",
    "print(x.shape, data['feature_names'])\n",
    "print(y.shape, data['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83ec83-f018-43d6-ab0f-90781717c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use dense layers with dropout. Dropout helps us prevent overfitting\n",
    "# when we're training.\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(30,)),\n",
    "    layers.Dense(20, activation='tanh'),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    layers.Dense(12, activation='tanh'),\n",
    "    layers.Dropout(rate=0.2),\n",
    "    layers.Dense(4, activation='tanh'),\n",
    "    # The last layer has a sigmoid activation function to make sure we're\n",
    "    # in the range [0,1]\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "# Binary cross-entropy is a good loss to select for classification problems\n",
    "model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=[\"binary_accuracy\"]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834efc36-75f0-4a36-9e39-0c8799b8adce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A validation split sets aside some of our data to monitor how well we're doing\n",
    "# on things the training loop hasn't seen directly. It's a proxy for testing error\n",
    "# we can use while still tuning our hyper-parameters with it.\n",
    "history = model.fit(x,y,epochs=100,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2344b-fa43-4c4e-8046-da6afb4e71a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The model predicts a float and we need to convert that to 0 or 1. We'll use a\n",
    "# threshold of 0.5, but often if you care about false positives or negatives more\n",
    "# you could pick a more or less conservative value.\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_pred = np.squeeze(model.predict(x))\n",
    "y_class = np.array([1 if prediction > threshold \n",
    "                    else 0 for prediction in y_pred\n",
    "                   ])\n",
    "\n",
    "# Let's look at the whole data, and then just the ones we get wrong. We'll\n",
    "# visualize with the first two input dimensions.\n",
    "_,axs = plt.subplots(2)\n",
    "axs[0].scatter(x[:,0],x[:,1],c=y,edgecolor='k',cmap='cool')\n",
    "axs[0].set_xlabel(data['feature_names'][0]);\n",
    "axs[0].set_ylabel(data['feature_names'][1]);\n",
    "axs[1].scatter(x[np.logical_not(y==y_class),0],x[np.logical_not(y==y_class),1],c=y_pred[np.logical_not(y==y_class)],edgecolor='k',cmap='cool')\n",
    "axs[1].set_xlim(axs[0].get_xlim())\n",
    "axs[1].set_ylim(axs[0].get_ylim())\n",
    "axs[1].set_xlabel(data['feature_names'][0]);\n",
    "axs[1].set_ylabel(data['feature_names'][1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341181a-b657-4014-b7ef-8c3954c0adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's see how we do on our testing data\n",
    "y_pred = np.squeeze(model.predict(x_test))\n",
    "y_class = np.array([1 if prediction > threshold \n",
    "                    else 0 for prediction in y_pred\n",
    "                   ])\n",
    "print(f'True: {y_test}')\n",
    "print(f'Pred: {y_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b7dbb-c170-4b5b-8749-30387ce962e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c420ec2-4d11-4b21-b5c5-d7127c4b23c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
